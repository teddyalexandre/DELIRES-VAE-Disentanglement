{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dataset: KeysView(<numpy.lib.npyio.NpzFile object at 0x7fb0fdf69c60>)\n",
      "Metadata: \n",
      " {b'date': b'April 2017', b'description': b'Disentanglement test Sprites dataset.Procedurally generated 2D shapes, from 6 disentangled latent factors.This dataset uses 6 latents, controlling the color, shape, scale, rotation and position of a sprite. All possible variations of the latents are present. Ordering along dimension 1 is fixed and can be mapped back to the exact latent values that generated that image.We made sure that the pixel outputs are different. No noise added.', b'version': 1, b'latents_names': (b'color', b'shape', b'scale', b'orientation', b'posX', b'posY'), b'latents_possible_values': {b'orientation': array([0.        , 0.16110732, 0.32221463, 0.48332195, 0.64442926,\n",
      "       0.80553658, 0.96664389, 1.12775121, 1.28885852, 1.44996584,\n",
      "       1.61107316, 1.77218047, 1.93328779, 2.0943951 , 2.25550242,\n",
      "       2.41660973, 2.57771705, 2.73882436, 2.89993168, 3.061039  ,\n",
      "       3.22214631, 3.38325363, 3.54436094, 3.70546826, 3.86657557,\n",
      "       4.02768289, 4.1887902 , 4.34989752, 4.51100484, 4.67211215,\n",
      "       4.83321947, 4.99432678, 5.1554341 , 5.31654141, 5.47764873,\n",
      "       5.63875604, 5.79986336, 5.96097068, 6.12207799, 6.28318531]), b'posX': array([0.        , 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n",
      "       0.16129032, 0.19354839, 0.22580645, 0.25806452, 0.29032258,\n",
      "       0.32258065, 0.35483871, 0.38709677, 0.41935484, 0.4516129 ,\n",
      "       0.48387097, 0.51612903, 0.5483871 , 0.58064516, 0.61290323,\n",
      "       0.64516129, 0.67741935, 0.70967742, 0.74193548, 0.77419355,\n",
      "       0.80645161, 0.83870968, 0.87096774, 0.90322581, 0.93548387,\n",
      "       0.96774194, 1.        ]), b'posY': array([0.        , 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n",
      "       0.16129032, 0.19354839, 0.22580645, 0.25806452, 0.29032258,\n",
      "       0.32258065, 0.35483871, 0.38709677, 0.41935484, 0.4516129 ,\n",
      "       0.48387097, 0.51612903, 0.5483871 , 0.58064516, 0.61290323,\n",
      "       0.64516129, 0.67741935, 0.70967742, 0.74193548, 0.77419355,\n",
      "       0.80645161, 0.83870968, 0.87096774, 0.90322581, 0.93548387,\n",
      "       0.96774194, 1.        ]), b'scale': array([0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), b'shape': array([1., 2., 3.]), b'color': array([1.])}, b'latents_sizes': array([ 1,  3,  6, 40, 32, 32]), b'author': b'lmatthey@google.com', b'title': b'dSprites dataset'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_zip = np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz', allow_pickle=True, encoding='bytes')\n",
    "\n",
    "print('Keys in the dataset:', dataset_zip.keys())\n",
    "imgs = dataset_zip['imgs']\n",
    "latents_values = dataset_zip['latents_values']\n",
    "latents_classes = dataset_zip['latents_classes']\n",
    "metadata = dataset_zip['metadata'][()]\n",
    "\n",
    "print('Metadata: \\n', metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DspritesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_zip):\n",
    "        self.imgs = dataset_zip['imgs']\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = DspritesDataset(dataset_zip)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, beta, z_dim, n_rows, n_cols, n_channels, n_filters, kernel_size):\n",
    "        super(BetaVAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.n_rows = n_rows\n",
    "        self.n_cols = n_cols\n",
    "        self.n_channels = n_channels\n",
    "        self.n_filters = n_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.beta = beta\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_filters, kernel_size, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_filters, n_filters, kernel_size, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_filters*2, n_filters*2, kernel_size, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(n_filters*2, n_filters*2, kernel_size, stride=2, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(4*4*n_filters*2, 256)\n",
    "        self.fc21 = nn.Linear(256, z_dim)\n",
    "        self.fc22 = nn.Linear(256, z_dim)\n",
    "\n",
    "        self.fc_reverse1 = nn.Linear(z_dim, 256)\n",
    "        self.fc_reverse2 = nn.Linear(256, 4*4*n_filters*2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(n_filters*2, n_filters*2, kernel_size, stride=2, padding=1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(n_filters*2, n_filters, kernel_size, stride=2, padding=1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(n_filters, n_filters, kernel_size, stride=2, padding=1)\n",
    "        self.upconv4 = nn.ConvTranspose2d(n_filters, n_channels, kernel_size, stride=2, padding=1)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(-1, 256)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc21(x)\n",
    "        logvar = self.fc22(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        z = F.relu(self.fc_reverse1(z))\n",
    "        z = F.relu(self.fc_reverse2(z))\n",
    "        z = z.view(-1, self.n_filters*2, 4, 4)\n",
    "        z = F.relu(self.upconv1(z))\n",
    "        z = F.relu(self.upconv2(z))\n",
    "        z = F.relu(self.upconv3(z))\n",
    "        z = self.upconv4(z)\n",
    "        return z\n",
    "    \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "    \n",
    "    def loss_function(self, y, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(y, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return BCE + self.beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training phase\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "beta = 4\n",
    "model = BetaVAE(4, 32, 64, 64, 3, 32, 4).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = model.loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(data_loader.dataset)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
